agent01.resources=r1
agent01.channels=c1
agent01.sinks=k1 k2

agent01.sinkgroups=g1
agent01.sinkgroups.g1.sinks=k1 k2
agent01.sinkgroups.g1.processor.type=failover
agent01.sinkgroups.g1.processor.priority.k1=5
agent01.sinkgroups.g1.processor.priority.k2=10
agent01.sinkgroups.g1.processor.maxpenalty=10000

agent01.resources.r1.type=TAILDIR
agent01.resources.r1.positionFile=/home/hadoop/data/flume/agent01/position/taildir_position.json
agent01.resources.r1.fileHeader=true
agent01.resources.r1.filegroups=g1 g2
agent01.resources.r1.filegroups.g1=/home/hadoop/data/flume/agent01/data1/*.log
agent01.resources.r1.filegroups.g2=/home/hadoop/data/flume/agent01/data2/*.log

agent01.resources.r1.interceptors=i1 i2 i3
agent01.resources.r1.interceptors.i1.type=timestamp
agent01.resoruces.r1.interceptors.i1.preserveExisting=false
agent01.resources.r1.interceptors.i2.type=host
agent01.resources.r1.interceptors.i2.preserveExisting=true
agent01.resources.r1.interceptors.i2.useIP=true
agent01.resources.r1.interceptors.i2.hostHeader=host
agent01.resources.r1.interceptors.i3.type=static
agent01.resources.r1.interceptors.i3.key=logtype
agent01.resources.r1.interceptors.i3.value=clickflow
agent01.resources.r1.interceptors.i3.preserveExisting=true

agent01.resources.r1.channels=c1


agent01.channels.c1.type=memory
agent01.channels.c1.capacity=10000
agent01.channels.c1.transactionCapacity=1000

agent01.sinks.k1.type=avro
agent01.sinks.k1.hostname=hadoop02
agent01.sinks.k1.port=8001
agent01.sinks.k1.batch-size=1000
agent01.sinks.k1.compression-level=8

agent01.sinks.k2.type=avro
agent01.sinks.k2.hostname=hadoop02
agent01.sinks.k2.port=8002
agent01.sinks.k2.batch-size=1000
agent01.sinks.k2.compression-level=8


